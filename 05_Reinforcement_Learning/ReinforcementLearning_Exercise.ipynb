{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d4f0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaaaf7d",
   "metadata": {},
   "source": [
    "## 6.1. Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3ce3e",
   "metadata": {},
   "source": [
    "For the problem formulation, we introduce the [gymnasium](https://gymnasium.farama.org/) library. It implements control problems from the past and present of reinforcement learning that have served as milestones in the development of that technique. Researchers that work on the same standard problems have the advantage that their work is easier to compare and to transfer. On the other hand, if benchmark problems are too prevalent in a community, it may drive research in a certain, uniform direction that is not as productive anymore. Note that gym is a product of OpenAI, a private company. \n",
    "\n",
    "gym uses a unifying framework that defines every control problem as an *environment*. The basic building blocks of an environment are `env = gym.make` to create the environment, `env.reset` to start an episode, `env.render` to give a human readable representation of the state of the environment, and `env.step` to perform an action.\n",
    "\n",
    "We start the exercises with the 4x4 [FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) environment. It is a kind of maze with \"frozen\" traversable squares marked by `F` and \"holes\", losing terminal squares marked by `H`. The agent starts at the `S` start square and only incurs reward, when they manage to get to the goal `G` square. We mostly look at the deterministic case, where traversing on the frozen lake is deterministic, which is controlled by the variable `is_slippery=False` when creating the environment. If the lake is slippery, a movement in a certain direction may by chance result in the agent arriving at a different square than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f3ef494",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
    "#print(env.action_space)\n",
    "#print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a0af69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state, _ = env.reset()\n",
    "#print(starting_state)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34850d84",
   "metadata": {},
   "source": [
    "The `env.action_space` always implements a `sample` method, which returns a valid, random aciton. We can utilize this, to have a look at the dynamics of the system. You can execute the following cell a few times to see what happens. When the agent enters a terminal state, you need to execute `env.reset` to start anew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47af7c8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(state, reward, terminated, truncated, info)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:208\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    210\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    211\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:309\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m, r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "print(state, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb143e",
   "metadata": {},
   "source": [
    "#### Task 1. a) Random Agent:\n",
    "We provide the framework for the random agent, a method to rollout a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb60f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, agent):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.action(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self, action_space, observation_space):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        \n",
    "    # We pass the state only for compatability\n",
    "    def action(self, state):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "def compute_avg_return(env, agent, num_episodes=5000):\n",
    "    avg_reward = 0\n",
    "    for _ in range(num_episodes):\n",
    "        avg_reward += rollout(env, agent)/num_episodes\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85a08a",
   "metadata": {},
   "source": [
    "Add your code to estimate the `avg_return_random_agent` for the deterministic case and `avg_return_random_agent_slippery` for the stochastic case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1754c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=None)\n",
    "env_slippery= gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=None)\n",
    "ra = RandomAgent(env.action_space, env.observation_space)\n",
    "avg_return_random_agent = compute_avg_return(env, ra)\n",
    "avg_return_random_agent_slippery = compute_avg_return(env_slippery, ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8e8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation for the deterministic case: 0.014800000000000013\n",
      "Estimation for the stochastic case: 0.01380000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimation for the deterministic case:\", avg_return_random_agent)\n",
    "print(\"Estimation for the stochastic case:\", avg_return_random_agent_slippery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c63c3",
   "metadata": {},
   "source": [
    "### 1. b) Iterative Policy Evaluation\n",
    "We provide a `set_state` method that changes the state of the environment. This is a pretty unusual way to interact with this framework. Note, that the random policy is stochastic, while the environment is not. In the value update we sum the value of each possible action that is weighted by its probability to be picked by the action. The architecture of the agent does provide access to these inner dynamics, so instead of passing the agent or its dynamics as a variable, we implement iterative policy evaluation just for the random agent, with the probability of `0.25` for each action hard coded.\n",
    "\n",
    "We also provide `all_states` and `all_actions`, lists of all admissable states and actions for the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce56d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = list(range(env.observation_space.n))\n",
    "all_actions = list(range(env.action_space.n))\n",
    "\n",
    "def set_state(env, state):\n",
    "    env.reset()\n",
    "    env.unwrapped.s = state\n",
    "    return env\n",
    "\n",
    "def visualize_value_fct(v):\n",
    "    print(np.round(np.array(list(v.values())).reshape((4,4)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a32c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_iteration_random_agent(env, all_states, all_actions, discount_rate, threshold = 0.001, max_iter = 10000):\n",
    "    V = {s: 0 for s in all_states}  # value function, initialized to 0\n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "        delta = 0\n",
    "        for s in all_states:\n",
    "            v = V[s]\n",
    "            V[s] = 0\n",
    "            for a in all_actions:\n",
    "                make_step = set_state(env, s).step(a)\n",
    "                make_step_state = make_step[0]\n",
    "                if make_step_state != s:\n",
    "                    V[s] += 0.25 * (make_step[1] + discount_rate * V[make_step_state])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        i += 1\n",
    "        done = i == max_iter or delta < threshold\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35557fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.001 0.004 0.001]\n",
      " [0.002 0.    0.018 0.   ]\n",
      " [0.008 0.035 0.076 0.   ]\n",
      " [0.    0.071 0.283 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "v_random = iterative_policy_iteration_random_agent(env, all_states, all_actions, discount_rate=0.9)\n",
    "visualize_value_fct(v_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00941b0",
   "metadata": {},
   "source": [
    "We should assume the same result for `env_slippery` because $3* 0.25 * 1/3 = 0.25$.\n",
    "\n",
    "### 1. c) Value Iteration\n",
    "Use value iteration to find the optimal policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61587995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, all_states, all_actions, discount_rate, threshold = 0.001, max_iter = 10000):\n",
    "    V = {s: 0 for s in all_states}  # value function, initialized to 0\n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "        delta = 0\n",
    "        for s in all_states:\n",
    "            v = V[s]\n",
    "            V[s] = 0\n",
    "            for a in all_actions:\n",
    "                make_step = set_state(env, s).step(a)\n",
    "                make_step_state = make_step[0]\n",
    "                if make_step_state != s:\n",
    "                    V[s] = max(V[s], make_step[1] + discount_rate * V[make_step_state])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        i += 1\n",
    "        done = i == max_iter or delta < threshold\n",
    "    \n",
    "    # retrieve optimal policy pi_optimal\n",
    "    array = np.zeros((len(all_states), len(all_actions)))\n",
    "    for s in range(len(all_states)):\n",
    "        for a in range(len(all_actions)):\n",
    "            make_step = set_state(env, s).step(a)\n",
    "            make_step_state = make_step[0]\n",
    "            if make_step_state != s:\n",
    "                array[s][a] = make_step[1] + 0.9 * V[make_step_state]\n",
    "    pi = {s: np.argmax(array[s]) for s in all_states}\n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59  0.656 0.729 0.656]\n",
      " [0.656 0.    0.81  0.   ]\n",
      " [0.729 0.81  0.9   0.   ]\n",
      " [0.    0.9   1.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "v_optimal, pi_optimal = value_iteration(env, all_states, all_actions, discount_rate=0.9)\n",
    "visualize_value_fct(v_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7026b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected return for learned policy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Expected return for learned policy\n",
    "\n",
    "def rollout(env):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state, reward, terminated, truncated, _ = env.step(pi_optimal[state])\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "print(\"Expected return for learned policy:\", rollout(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5617c",
   "metadata": {},
   "source": [
    "### 2. a) Sarsa & Q-Learning\n",
    "With the language of a Q-table, we can define a more general agent by a Q-function.\n",
    "\n",
    "*Please do not use* `set_state` *anymore! Instead always start an episode with* `state = env.reset()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc7e6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_fct(q):\n",
    "    acts = {0 : \"L\", 1 : \"D\", 2 : \"R\", 3 : \"U\"} \n",
    "    for j in range(4):\n",
    "        print(\"Value for action\", acts[j], \":\")\n",
    "        print(np.round(np.array([q[i][j] for i in range(16)]).reshape((4,4)), 3))\n",
    "    for i in range(4):\n",
    "        print([acts[np.argmax(q[4*i + j])] for j in range(4)])\n",
    "        \n",
    "def argmax_tiebreak(array):\n",
    "    return np.random.choice(np.where(array == array.max())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "904ec408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discrete_Q_Agent:\n",
    "    def __init__(self, action_space, observation_space, epsilon=0.9):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.epsilon = epsilon\n",
    "        self.reset_Q()\n",
    "    \n",
    "    def reset_Q(self):\n",
    "        all_states = list(range(self.observation_space.n))\n",
    "        self.actions = list(range(self.action_space.n))\n",
    "        self.Q = {s: np.zeros(self.action_space.n) for s in all_states}\n",
    "\n",
    "    def action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            action = argmax_tiebreak(self.Q[state])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88b55655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env, q_agent, alpha=0.1, gamma=0.99, rollouts=10000):\n",
    "    for i in range(rollouts):\n",
    "        state, _ = env.reset()\n",
    "        action = q_agent.action(state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_prime, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            action_prime = q_agent.action(state_prime)\n",
    "            q_agent.Q[state][action] += alpha * (reward + gamma * q_agent.Q[state_prime][action_prime] - q_agent.Q[state][action])\n",
    "            state, action = state_prime, action_prime\n",
    "    return q_agent, q_agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bfadf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning(env, q_agent, alpha=0.1, gamma=0.99, rollouts=10000):\n",
    "    for i in range(rollouts):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = q_agent.action(state)\n",
    "            state_prime, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            q_agent.Q[state][action] += alpha * (reward + gamma * q_agent.Q[state_prime][argmax_tiebreak(q_agent.Q[state_prime])] - q_agent.Q[state][action])\n",
    "            state = state_prime\n",
    "    return q_agent, q_agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7993db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for action L :\n",
      "[[0.019 0.01  0.038 0.013]\n",
      " [0.031 0.    0.025 0.   ]\n",
      " [0.022 0.053 0.189 0.   ]\n",
      " [0.    0.057 0.211 0.   ]]\n",
      "Value for action D :\n",
      "[[0.019 0.011 0.018 0.014]\n",
      " [0.022 0.    0.062 0.   ]\n",
      " [0.045 0.117 0.083 0.   ]\n",
      " [0.    0.258 0.631 0.   ]]\n",
      "Value for action R :\n",
      "[[0.02  0.014 0.025 0.009]\n",
      " [0.024 0.    0.064 0.   ]\n",
      " [0.067 0.14  0.073 0.   ]\n",
      " [0.    0.248 0.51  0.   ]]\n",
      "Value for action U :\n",
      "[[0.016 0.02  0.015 0.016]\n",
      " [0.013 0.    0.01  0.   ]\n",
      " [0.048 0.07  0.034 0.   ]\n",
      " [0.    0.166 0.735 0.   ]]\n",
      "['R', 'U', 'L', 'U']\n",
      "['L', 'L', 'R', 'L']\n",
      "['R', 'R', 'L', 'L']\n",
      "['L', 'D', 'U', 'L']\n"
     ]
    }
   ],
   "source": [
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "q_agent = Discrete_Q_Agent(env_slippery.action_space, env_slippery.observation_space, epsilon=0.9)\n",
    "q_agent, q = Sarsa(env_slippery, q_agent)\n",
    "visualize_q_fct(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e3afe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected return for learned policy: 0.106600000000001\n"
     ]
    }
   ],
   "source": [
    "# Expected return for learned policy\n",
    "\n",
    "def rollout(env, agent):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.action(state)\n",
    "        state, reward, terminated, truncated, info = env.step(argmax_tiebreak(agent.Q[state]))\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "print(\"Expected return for learned policy:\", compute_avg_return(env_slippery, q_agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91535fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for action L :\n",
      "[[0.628 0.408 0.473 0.3  ]\n",
      " [0.642 0.    0.349 0.   ]\n",
      " [0.513 0.464 0.694 0.   ]\n",
      " [0.    0.453 0.797 0.   ]]\n",
      "Value for action D :\n",
      "[[0.608 0.296 0.468 0.346]\n",
      " [0.363 0.    0.232 0.   ]\n",
      " [0.473 0.754 0.61  0.   ]\n",
      " [0.    0.615 0.872 0.   ]]\n",
      "Value for action R :\n",
      "[[0.598 0.263 0.456 0.326]\n",
      " [0.506 0.    0.348 0.   ]\n",
      " [0.501 0.557 0.442 0.   ]\n",
      " [0.    0.804 0.859 0.   ]]\n",
      "Value for action U :\n",
      "[[0.603 0.556 0.504 0.487]\n",
      " [0.407 0.    0.178 0.   ]\n",
      " [0.666 0.437 0.426 0.   ]\n",
      " [0.    0.644 0.775 0.   ]]\n",
      "['L', 'U', 'U', 'U']\n",
      "['L', 'L', 'L', 'L']\n",
      "['U', 'D', 'L', 'L']\n",
      "['L', 'R', 'D', 'L']\n"
     ]
    }
   ],
   "source": [
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "q_agent = Discrete_Q_Agent(env_slippery.action_space, env_slippery.observation_space, epsilon=0.9)\n",
    "q_agent, q = Q_Learning(env_slippery, q_agent)\n",
    "visualize_q_fct(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b7da610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected return for learned policy: 0.7467999999999504\n"
     ]
    }
   ],
   "source": [
    "#Expected return for learned policy\n",
    "\n",
    "print(\"Expected return for learned policy:\", compute_avg_return(env_slippery, q_agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4eab0e",
   "metadata": {},
   "source": [
    "### 2. b) Cartpole\n",
    "Next, try the [Cartpole](https://www.gymlibrary.ml/environments/classic_control/cart_pole/) environment. It has a continuous state space, so we need to adjust our methods to accomodate that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95f036dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/.local/lib/python3.12/site-packages/gymnasium/envs/classic_control/cartpole.py:180: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "def random_policy(env):\n",
    "    for _ in range(200):\n",
    "        env.step(env.action_space.sample())\n",
    "\n",
    "random_policy(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac79b2d8",
   "metadata": {},
   "source": [
    "### 2. c) Cartpole learning\n",
    "The observation space of the Cartpole environment can be accessed with `env.observation_space`. It is a [`Box`](https://gymnasium.farama.org/api/spaces/fundamental/#box) space, which contains lower bounds, upper bounds, number of dimensions, and datatype. The second and forth dimension are unbounded. We can make them bounded by clipping every value over a certain threshold. Also, the first and third dimension have higher admissbable bounds, than is useful during training!\n",
    "\n",
    "Hint: Binned Q-Learning is not the most efficient or useful algorithm for this problem. With the provided hyperparameters I achieved only a mean reward of ~100 after 50000 rollouts of training without any further tuning. Can you achieve a better result by changing the hyperparameters or employing some additional technique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b052a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "discounting_rate = 0.95\n",
    "number_episodes = 50000\n",
    "total_reward = 0\n",
    "\n",
    "q_table = np.zeros([31, 31, 51, 51, 2])\n",
    "window_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "low_clip = [-3.75, -3.75, -0.25, -2.5]\n",
    "high_clip = [3.75, 3.75, 0.25, 2.5]\n",
    "\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e73ee792",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "#bagent = Binned_Q_Agent_Cartpole(window_size, q_table)\n",
    "#binned_q_learning(env, bagent, num_episodes=50000)\n",
    "def norm_state(state):\n",
    "    norm_state = state\n",
    "    norm_state = np.clip(norm_state,low_clip,high_clip)\n",
    "    norm_state /= high_clip\n",
    "    return norm_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93474134",
   "metadata": {},
   "source": [
    "### 3.a) Linear function control\n",
    "Implement the linear gradient Sarsa here. Most of the time after a few thousend episodes the linear policy is able to solve the problem (500 reward), but sometimes it just does not converge. The algorithm is a bit shakey as is! I also needed to add one little tweak: Normalize the state by clipping it, just as in the task before, and then dividing by the clip-value. This normalizes the state-vectors to [-1,1] and stablizes the algorithm.\n",
    "\n",
    "Note that for a linear formulation of Q_theta(., a), Grad(Q_theta(., a)) at state vector s is just that state vector s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a70c03df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Q_Agent:\n",
    "    def __init__(self, action_space, observation_space, epsilon=0.9):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.epsilon = epsilon\n",
    "        self.theta = np.zeros((action_space.n, observation_space.shape[0]))\n",
    "    def Q_theta(self,state):\n",
    "        return np.dot(self.theta, state)\n",
    "    \n",
    "    def action(self,state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            return argmax_tiebreak(self.Q_theta(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb68a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Sarsa(env, agent, rollouts, alpha = 0.1, gamma = 0.99):\n",
    "    for _ in range(rollouts):\n",
    "        s, _ = env.reset()\n",
    "        a = agent.action(s)\n",
    "        done = False\n",
    "        while not done:\n",
    "            s_new, reward, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                agent.theta += alpha * (reward - agent.Q_theta(s)[a]) * s\n",
    "            else:\n",
    "                a_new = agent.action(s_new)\n",
    "                agent.theta += alpha * (reward + gamma * agent.Q_theta(s_new)[a_new] - agent.Q_theta(s)[a]) * s\n",
    "                s = s_new\n",
    "                a = a_new\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d16fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Sarsa(env, agent, rollouts, alpha = 0.1, gamma = 0.99):\n",
    "    for _ in range(rollouts):\n",
    "        s = norm_state(env.reset()[0])\n",
    "        a = agent.action(s)\n",
    "        done = False\n",
    "        while not done:\n",
    "            s_new, reward, terminated, truncated, _ = env.step(a)\n",
    "            s_new = norm_state(s_new)\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                agent.theta += alpha * (reward - agent.Q_theta(s)[a]) * s\n",
    "            else:\n",
    "                a_new = agent.action(s_new)\n",
    "                agent.theta += alpha * (reward + gamma * agent.Q_theta(s_new)[a_new] - agent.Q_theta(s)[a]) * s\n",
    "                s = s_new\n",
    "                a = a_new\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "330b4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_agent = Linear_Q_Agent(env.action_space, env.observation_space)\n",
    "lin_agent = Grad_Sarsa(env, lin_agent, rollouts=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae0b1e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected return for learned policy: 22.103199999999873\n"
     ]
    }
   ],
   "source": [
    "# Expected return for learned policy\n",
    "\n",
    "def rollout(env, agent):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state, reward, terminated, truncated, _ = env.step(argmax_tiebreak(agent.Q_theta(state)))\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "print(\"Expected return for learned policy:\", compute_avg_return(env, lin_agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0af6cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render one rollout (of learned policy)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "s, _ = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    state, reward, terminated, truncated, _ = env.step(argmax_tiebreak(lin_agent.Q_theta(s)))\n",
    "    done = terminated or truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9638e6",
   "metadata": {},
   "source": [
    "### 3.b) DQN\n",
    "As a suggestion, I provided the interfaces for functions, some hyperparameters, and the architecture of the neural net that approximates Q. For this algorithm to somewhat work, I needed at least experience replay. But other techniques may also be interesting and work even better. Please feel free to experiment!\n",
    "\n",
    "*Note*: 1. Whenever you either `model.predict` oder `model.fit` you can gain a lot of performance if you do it as a batch. E.g. use \n",
    "```\n",
    "X = []\n",
    "y = []\n",
    "for i in I:\n",
    "    X.append(get_data(i))\n",
    "    y.append(get_label(i))\n",
    "model.fit(X,y)\n",
    "```\n",
    "instead of\n",
    "```\n",
    "for i in I:\n",
    "    model.fit(get_data(i), get_label(i))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de617670",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_size = 2000\n",
    "epsilon = 0.05\n",
    "learning_rate = 0.001\n",
    "\n",
    "class DQN_Agent:\n",
    "    def _init_model(self, state_dim, action_dim, learning_rate):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=state_dim, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(action_dim, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n",
    "        return model\n",
    "        \n",
    "    def action(self, state):\n",
    "        pass\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        pass\n",
    "\n",
    "    def learn_from_replay(self, batch_size):\n",
    "        pass\n",
    "    \n",
    "def DQN(env, agent, replay_batch_size=128, rollouts=2000):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f78b5b",
   "metadata": {},
   "source": [
    "### 3.c) Another one\n",
    "Browse the [environments](https://gymnasium.farama.org/) to pick another challenge! Maybe even record a video with the [RecordVideo wrapper](https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.RecordVideo)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c377104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
